{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebooks is inspired by a [GPT-2 fine-tuning tutorial](https://towardsdatascience.com/fine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRHEjKRJbNjY"
      },
      "source": [
        "## **What are we going to do:**\n",
        "\n",
        "- load the dataset from kaggle\n",
        "- prepare the dataset and build a ``TextDataset``\n",
        "- load the pre-trained GPT-2 model and tokenizer\n",
        "- initialize ``Trainer`` with ``TrainingArguments``\n",
        "- train and save the model\n",
        "- test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7_e3GBMPX_M",
        "outputId": "6ac9c641-8d87-47bb-bd26-ca58c601573d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "import re\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "\n",
        "datadir = \"wordlists/\"\n",
        "path = 'reviews_glove.txt'\n",
        "!mkdir GPT2_ft_WEAT_results\n",
        "results_folder = 'GPT2_ft_WEAT_results'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH0JtdCeE1n2",
        "outputId": "43265ac0-0819-4094-f0e5-8e9ae2aa7921",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuvcS_wFbB3e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "with open(path, encoding='UTF8') as f:\n",
        "    contents = f.read()\n",
        "\n",
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('<.*?>')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  cleantext = re.sub(r'http\\S+', '', cleantext)\n",
        "  cleantext = cleantext.translate(str.maketrans('', '', string.punctuation))\n",
        "  return cleantext.split('\\n')\n",
        "\n",
        "cleaned_contents = cleanhtml(contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S401ybVtm0f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "output_file = open('cleaned_contents_eng.txt', 'w')\n",
        "\n",
        "for i in cleaned_contents:\n",
        "  if len(i) > 5000:\n",
        "    i_1 = i[0:4000]\n",
        "    i_1 = trans.translate(i_1)\n",
        "    i_2 = i[4000:]\n",
        "    i_2 = trans.translate(i_2)\n",
        "    output_file.write(i_1 +\" \" + i_2 + \"\\n\")\n",
        "  else:\n",
        "    output_file.write(trans.translate(i) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQBcQuZ90G7S",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "output_file = open('cleaned_contents_eng.txt', 'w')\n",
        "\n",
        "for text in cleaned_contents_eng:\n",
        "    output_file.write(text + \"\\n\")\n",
        "\n",
        "output_file = open('cleaned_contents.txt', 'w')\n",
        "\n",
        "for text in cleaned_contents:\n",
        "    output_file.write(text + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V36gOIOfLHvB",
        "outputId": "38050553-fcd5-44d5-e692-1159da18e954",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def build_text_files(data_json, dest_path):\n",
        "    f = open(dest_path, 'w')\n",
        "    data = data_json\n",
        "    for texts in data_json:\n",
        "        f.write(texts + \" \")\n",
        "    \n",
        "train, test = train_test_split(cleaned_contents,test_size=0.15) \n",
        "\n",
        "\n",
        "build_text_files(train,'train_dataset.txt')\n",
        "build_text_files(test,'test_dataset.txt')\n",
        "\n",
        "print(\"Train dataset length: \"+str(len(train)))\n",
        "print(\"Test dataset length: \"+ str(len(test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg78wNnTl305"
      },
      "source": [
        "the next step is to download the tokenizer, which we use. We use the tokenizer from the `german-gpt2` model on [huggingface](https://huggingface.co/anonymous-german-nlp/german-gpt2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCaunLMtlPfw",
        "outputId": "b2f096cb-65ed-410f-e9b7-32f2e39766e9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"anonymous-german-nlp/german-gpt2\")\n",
        "\n",
        "train_path = 'train_dataset.txt'\n",
        "test_path = 'test_dataset.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9lHS0mIMak4",
        "outputId": "04a91a7c-5098-49f3-a64c-3e5a2d8d8ecd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "\n",
        "def load_dataset(train_path,test_path,tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "     \n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)   \n",
        "    \n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        "\n",
        "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThG6jwL7qET8"
      },
      "source": [
        "# Initialize `Trainer` with `TrainingArguments` and GPT-2 model\n",
        "\n",
        "The [Trainer](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer) class provides an API for feature-complete training. It is used in most of the [example scripts](https://huggingface.co/transformers/examples.html) from Huggingface. Before we can instantiate our `Trainer` we need to download our GPT-2 model and create a [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments) to access all the points of customization during training. In the `TrainingArguments`, we can define the Hyperparameters we are going to use in the training process like our `learning_rate`, `num_train_epochs`, or  `per_device_train_batch_size`. A complete list can you find [here](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "b4052e8352b84bb095b0ddd274ea0caa",
            "b27e43aaebd243a286b0f47ed9fe3515",
            "c0425becc3a14fc9a9ed1e0b5eafdc9f",
            "927e13f1feec4dd886ea47445e30eaa7",
            "2c9a242a5bc041a8a8859f7edb5bbb63",
            "962f31f7eeb34fbe8fb49b3e411a0892",
            "ba63820eff8c4a5e97e473dcaff87ea9",
            "7ad9bd00ee22492fac41027cc5bea9c8",
            "74b7d6a402dc4eeb8f1ba4a572929c4e",
            "79314b959d4842da8841b69d411669cd",
            "0113545a40914d959af6e1df749165f0"
          ]
        },
        "id": "H7hhmbT2ModI",
        "outputId": "728c237d-4aa6-495d-eb5a-d8585ed06385",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"anonymous-german-nlp/german-gpt2\")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-ft\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=3, # number of training epochs\n",
        "    per_device_train_batch_size=32, # batch size for training\n",
        "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=800, # after # steps model is saved \n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyQh0jlTqR6-"
      },
      "source": [
        "# Train and save the model\n",
        "\n",
        "To train the model we can simply run `Trainer.train()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "wjIzSWPTKzBf",
        "outputId": "993b16b8-2aad-4bb6-b277-b732dbcbc1d5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHXqTv8kqxpJ"
      },
      "source": [
        "After training is done you can save the model by calling `save_model()`. This will save the trained model to our `output_dir` from our `TrainingArguments`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5quyGeMNdjE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trF6oqm_b0Vq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('gpt2-ft')\n",
        "checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGEiQ1mhOyNv"
      },
      "source": [
        "# Test the model\n",
        "\n",
        "To test the model we are going to use another [highlight of the transformers library](https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines) called `pipeline`. [Pipelines](https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines) are objects that offer a simple API dedicated to several tasks, among others also `text-generation`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgQmK1z2lMq2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vec1, len1, vec2, len2):\n",
        "    \"\"\"calculates the cosine similarity between two vectors\"\"\"\n",
        "    if type(len1) == list or type(len2) == list:\n",
        "        return 0\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    return dot_product / (len1 * len2)\n",
        "\n",
        "\n",
        "trans = GoogleTranslator('de','en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh4Q1BR0OtDo",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def loadwordlist(test_num):\n",
        "    \"\"\"loads and returns all words in the given file.  Omits words not in the\n",
        "    word embedding matrix\"\"\"\n",
        "    if test_num > 9:\n",
        "        print('Invalid WEAT Test.')\n",
        "        return\n",
        "    with open(\"weat_tests_german.json\") as f:\n",
        "        weat_german = json.load(f)\n",
        "        test_string = \"test\" + str(test_num)\n",
        "        weat_test = weat_german[test_string]\n",
        "    return weat_test['X'], weat_test['Y'], weat_test['A'], weat_test['B']\n",
        "\n",
        "def loadwordnames(test_num):\n",
        "    if test_num > 9:\n",
        "        print('Invalid WEAT Test.')\n",
        "        return\n",
        "    with open(\"weat_tests_german.json\") as f:\n",
        "        weat_german = json.load(f)\n",
        "        test_string = \"test\" + str(test_num)\n",
        "        weat_test = weat_german[test_string]\n",
        "        attribute_names = weat_test['attribute_words'].split(' vs. ')\n",
        "        word_names = weat_test['target_words'].split(' vs. ')\n",
        "    return word_names[0], word_names[1], attribute_names[0], attribute_names[1]\n",
        "\n",
        "def getAverageSimilarity(targetVec, targetLength, attrVectors, attrLengths):\n",
        "    \"\"\"Calculates average similarity between words in the target list and attribute\n",
        "    list\"\"\"\n",
        "    return np.average([cosine_similarity(targetVec, targetLength, attrVectors[i], attrLengths[i]) for i in range(attrLengths.shape[0])])\n",
        "\n",
        "def getListData(conceptWords, tokenizer, transformer):\n",
        "    vectors = []\n",
        "    lengths = []\n",
        "    biggest_shape = []\n",
        "    for target_idx in range(len(conceptWords)):\n",
        "        if len(conceptWords[target_idx]) > 0:\n",
        "            if 'bert' in model_name:\n",
        "                word_id = tokenizer.encode(conceptWords[target_idx])[1]\n",
        "                v = transformer.embeddings.word_embeddings.weight[word_id].detach().numpy()\n",
        "            if 't5' in model_name:\n",
        "                word_id = tokenizer(conceptWords[target_idx], return_tensors=\"pt\")\n",
        "\n",
        "                # forward pass through encoder only\n",
        "                output = transformer.encoder(\n",
        "                    input_ids=word_id[\"input_ids\"], \n",
        "                    attention_mask=word_id[\"attention_mask\"], \n",
        "                    return_dict=True\n",
        "                )\n",
        "                # get the final hidden states\n",
        "                v = output.last_hidden_state.detach().numpy()\n",
        "                biggest_shape.append(v.shape)\n",
        "            if 'gpt2' in model_name:\n",
        "                word_id = tokenizer.encode(conceptWords[target_idx])\n",
        "                v = model.transformer.wte.weight[word_id,:].cpu().detach().numpy()\n",
        "                biggest_shape.append(v.shape)\n",
        "            vectors.append(v)\n",
        "            lengths.append(np.linalg.norm(v))\n",
        "    if 't5' in model_name:\n",
        "        biggest_shape = np.maximum.reduce([np.array(b) for b in biggest_shape])\n",
        "        padded_vectors = []\n",
        "        for v in vectors:\n",
        "            vec_zeros = np.zeros(biggest_shape)\n",
        "            vec_zeros[:v.shape[0],:v.shape[1],:v.shape[2]] = v\n",
        "            padded_vectors.append(vec_zeros.reshape(-1))\n",
        "        vectors = padded_vectors\n",
        "    if 'gpt2' in model_name:\n",
        "        biggest_shape = np.maximum.reduce([np.array(b) for b in biggest_shape])\n",
        "        padded_vectors = []\n",
        "        for v in vectors:\n",
        "            vec_zeros = np.zeros(biggest_shape)\n",
        "            vec_zeros[:v.shape[0],:v.shape[1]] = v\n",
        "            padded_vectors.append(vec_zeros.reshape(-1))\n",
        "        vectors = padded_vectors\n",
        "    vectors = np.array(vectors)\n",
        "    lengths = np.array(lengths)\n",
        "    return (vectors, lengths)\n",
        "\n",
        "def rankAttributes(targetData, targetLengths, attrData, attrLengths, attrWords, n= 5):\n",
        "    \"\"\"Return the n highest similarity scores between the target and all attr\"\"\"\n",
        "    attrSims = [getAverageSimilarity(attrData[i], attrLengths[i], targetData, targetLengths) for i in range(attrLengths.shape[0])]\n",
        "    return attrWords[np.argsort(attrSims)[-n:]]\n",
        "\n",
        "def run_weat(rundirect, male_word_lists=False):\n",
        "    #parse inputs, load in glove vectors and wordlists\n",
        "    tokenizer = AutoTokenizer.from_pretrained(rundirect[0])\n",
        "    model = BertModel.from_pretrained(rundirect[0])\n",
        "\n",
        "    test_num = rundirect[1]\n",
        "\n",
        "    target1,target2,attribute1,attribute2 = loadwordlist(test_num)\n",
        "    target1Name,target2Name,attribute1Name,attribute2Name = loadwordnames(test_num)\n",
        "\n",
        "    print('WEAT Test: ', test_num)\n",
        "    print('targets: ', target1Name, target2Name)\n",
        "    print('attributes: ', attribute1Name, attribute2Name)\n",
        "\n",
        "    target1Vecs, target1Lengths = getListData(target1, tokenizer, model)\n",
        "    target2Vecs, target2Lengths = getListData(target2, tokenizer, model)\n",
        "    attr1Data, attr1Lengths = getListData(attribute1, tokenizer, model)\n",
        "    attr2Data, attr2Lengths = getListData(attribute2, tokenizer, model)\n",
        "    \n",
        "    print(len(target1Vecs), len(target1Lengths))\n",
        "    \n",
        "    #Find more similar attribute words for each target list\n",
        "    print()\n",
        "    print(\"Top 5 most similar attribute words to %s:\" % target1Name)\n",
        "\n",
        "    topWordsT1 = rankAttributes(target1Vecs, target1Lengths, np.concatenate([attr1Data, attr2Data]),\n",
        "            np.concatenate([attr1Lengths, attr2Lengths]), np.array([attribute1[0], attribute2[0]]))\n",
        "    for word in topWordsT1[::-1]:\n",
        "        print(\"\\t\"+word)\n",
        "\n",
        "    print()\n",
        "    print(\"Top 5 most similar attribute words to %s:\" % target2Name)\n",
        "\n",
        "    topWordsT2 = rankAttributes(target1Vecs, target1Lengths, np.concatenate([attr1Data, attr2Data]),\n",
        "            np.concatenate([attr1Lengths, attr2Lengths]), np.array([attribute1[1], attribute2[1]]))\n",
        "    for word in topWordsT2[::-1]:\n",
        "        print(\"\\t\"+word)\n",
        "\n",
        "    print()\n",
        "    #calculate similarities between target 1 and both attributes\n",
        "    targ1attr1Sims = [getAverageSimilarity( target1Vecs[i], target1Lengths[i], attr1Data, attr1Lengths)\n",
        "        for i in range( target1Vecs.shape[0])]\n",
        "    targ1attr2Sims = [getAverageSimilarity( target1Vecs[i], target1Lengths[i], attr2Data, attr2Lengths)\n",
        "        for i in range( target1Vecs.shape[0])]\n",
        "    targ1SimDiff = np.subtract(targ1attr1Sims, targ1attr2Sims)\n",
        "\n",
        "    #calculate similarities between target 2 and both attributes\n",
        "    targ2attr1Sims = [getAverageSimilarity( target2Vecs[i], target2Lengths[i], attr1Data, attr1Lengths)\n",
        "        for i in range( target2Vecs.shape[0])]\n",
        "    targ2attr2Sims = [getAverageSimilarity( target2Vecs[i], target2Lengths[i], attr2Data, attr2Lengths)\n",
        "        for i in range( target2Vecs.shape[0])]\n",
        "    targ2SimDiff = np.subtract(targ2attr1Sims, targ2attr2Sims)\n",
        "\n",
        "    #effect size is avg difference in similarities divided by standard dev\n",
        "    d = (np.average(targ1SimDiff) - np.average(targ2SimDiff))/np.std(np.concatenate((targ1SimDiff,targ2SimDiff)))\n",
        "\n",
        "\n",
        "    print()\n",
        "    print(\"Calculating effect size.  The score is between +2.0 and -2.0.  \")\n",
        "    print(\"Positive scores indicate that %s is more associated with %s than %s.\" % (target1Name, attribute1Name, target2Name))\n",
        "    print(\"Or, equivalently, %s is more associated with %s than %s.\" % (target2Name, attribute2Name, target1Name))\n",
        "    print(\"Negative scores have the opposite relationship.\")\n",
        "    print(\"Scores close to 0 indicate little to no effect.\")\n",
        "    print()\n",
        "    print(\"Effect size: %.2f\" % d)\n",
        "\n",
        "    print()\n",
        "    print(\"Plotting similarity scores...\")\n",
        "\n",
        "    fig = plt.figure(figsize=(16,8))\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    ax1.set_title(\"Similarities Scores for Target/Attribute Pairs\")\n",
        "    ax2.set_title(\"Difference Scores For Each Target\")\n",
        "\n",
        "    # Box plot of pairwise similarity scores\n",
        "    df = pd.DataFrame()\n",
        "    df[\"Similarity\"] = np.concatenate([targ1attr1Sims,targ1attr2Sims,targ2attr1Sims,targ2attr2Sims])\n",
        "    df[\"Pairs\"] = [target1Name+\"-\"+attribute1Name]*len(targ1attr1Sims)+[target1Name+\"-\"+attribute2Name]*len(targ1attr2Sims)+[target2Name+\"-\"+attribute1Name]*len(targ2attr1Sims) \\\n",
        "      +[target2Name+\"-\"+attribute2Name]*len(targ2attr2Sims)\n",
        "    df[\"Target\"] = [target1Name]*len(targ1attr1Sims+targ1attr2Sims)+[target2Name]*len(targ2attr1Sims+targ2attr2Sims)\n",
        "    df[\"Attribute\"] = [attribute1Name]*len(targ1attr1Sims)+[attribute2Name]*len(targ1attr2Sims)+[attribute1Name]*len(targ2attr1Sims) +[attribute2Name]*len(targ2attr2Sims)\n",
        "    sns.boxplot(x=\"Target\", y=\"Similarity\", hue=\"Attribute\",data=df, ax=ax1)\n",
        "    #Box plot of target bias in similarities\n",
        "    df = pd.DataFrame()\n",
        "    df[\"Difference\"] = np.concatenate([targ1SimDiff, targ2SimDiff])\n",
        "    df[\"Target\"] = [target1Name]*len(targ1SimDiff) + [target2Name]*len(targ2SimDiff)\n",
        "    ax = sns.boxplot(x=\"Target\", y=\"Difference\", data=df, ax=ax2)\n",
        "\n",
        "\n",
        "    ticks = ax1.get_yticks()\n",
        "    mx = max(abs(ticks[0]),ticks[-1])\n",
        "    mx = int(mx*10+.99)/10.0\n",
        "    ax1.yaxis.set_ticks(np.arange(-mx,mx+.1,.1))\n",
        "    ticks = ax2.get_yticks()\n",
        "    mx = max(abs(ticks[0]),ticks[-1])\n",
        "    mx = int(mx*10+.99)/10.0\n",
        "    ax2.yaxis.set_ticks(np.arange(-mx,mx+.1,.1))\n",
        "\n",
        "    fig.subplots_adjust(wspace=0.5)\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    labels = [item.get_text() for item in ax1.get_yticklabels()]\n",
        "    labels[0] = \"(less similar) \" + labels[0]\n",
        "    labels[-1] = \"(more similar) \" + labels[-1]\n",
        "    ax1.set_yticklabels(labels)\n",
        "    labels = [item.get_text() for item in ax2.get_yticklabels()]\n",
        "    labels[0] = \"(%s) \" % attribute2Name + labels[0]\n",
        "    labels[len(labels)//2] = \"(neutral) 0.0\"\n",
        "    labels[-1] = \"(%s) \" % attribute1Name + labels[-1]\n",
        "    ax2.set_yticklabels(labels)\n",
        "\n",
        "    return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XsfxsNUreTU2",
        "outputId": "acd12249-106b-4348-cbe7-5920b20612d9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for i in range(1,10):\n",
        "    rundirect = ['gpt2_ft', i]\n",
        "    test_name = 'GPT2_ft_WEAT_results/gpt2_WEAT_test_ft_'+str(rundirect[1])\n",
        "\n",
        "    orig_stdout = sys.stdout\n",
        "    f = open(test_name +'.txt', 'w')\n",
        "    sys.stdout = f\n",
        "    \n",
        "    model_name = rundirect[0]\n",
        "    if 'bert' in model_name:\n",
        "        if 'ft' not in model_name:\n",
        "            tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
        "            model = BertModel.from_pretrained('bert-base-german-cased')\n",
        "    if 't5' in model_name:\n",
        "        if 'ft' not in model_name:\n",
        "          tokenizer = AutoTokenizer.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n",
        "          model = AutoModelForSeq2SeqLM.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n",
        "    if 'gpt2' in model_name:\n",
        "        if 'ft' not in model_name:\n",
        "          tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/german-gpt2\")\n",
        "          model = AutoModelForCausalLM.from_pretrained(\"dbmdz/german-gpt2\")\n",
        "\n",
        "    test_num = rundirect[1]\n",
        "\n",
        "    target1,target2,attribute1,attribute2 = loadwordlist(test_num)\n",
        "    target1Name,target2Name,attribute1Name,attribute2Name = loadwordnames(test_num)\n",
        "\n",
        "    print('WEAT Test: ', test_num)\n",
        "    print('targets: ', target1Name, target2Name)\n",
        "    print('attributes: ', attribute1Name, attribute2Name)\n",
        "\n",
        "    target1Vecs, target1Lengths = getListData(target1, tokenizer, model)\n",
        "    target2Vecs, target2Lengths = getListData(target2, tokenizer, model)\n",
        "    attr1Data, attr1Lengths = getListData(attribute1, tokenizer, model)\n",
        "    attr2Data, attr2Lengths = getListData(attribute2, tokenizer, model)\n",
        "    \n",
        "    print([target1Vecs.shape, target2Vecs.shape, attr1Data.shape, attr2Data.shape])\n",
        "\n",
        "    if 't5' or 'gpt2' in model_name:\n",
        "        max_shape = np.maximum.reduce([target1Vecs.shape, target2Vecs.shape, attr1Data.shape, attr2Data.shape])\n",
        "        \n",
        "        zero_attr = np.zeros(max_shape)\n",
        "        zero_attr[:target1Vecs.shape[0],:target1Vecs.shape[1]] = target1Vecs\n",
        "        target1Vecs = zero_attr\n",
        "\n",
        "        zero_attr = np.zeros(max_shape)\n",
        "        zero_attr[:target2Vecs.shape[0],:target2Vecs.shape[1]] = target2Vecs\n",
        "        target2Vecs = zero_attr\n",
        "\n",
        "        zero_attr = np.zeros(max_shape)\n",
        "        zero_attr[:attr2Data.shape[0], :attr2Data.shape[1]] = attr2Data\n",
        "        attr2Data = zero_attr\n",
        "\n",
        "        zero_attr = np.zeros(max_shape)\n",
        "        zero_attr[:attr1Data.shape[0], :attr1Data.shape[1]] = attr1Data\n",
        "        attr1Data = zero_attr\n",
        "\n",
        "#     Find more similar attribute words for each target list\n",
        "    print(\"Top 5 most similar attribute words to %s:\" % target1Name+ \", \" + trans.translate(target1Name))\n",
        "\n",
        "    topWordsT1 = rankAttributes(target1Vecs, target1Lengths, np.concatenate([attr1Data, attr2Data]),\n",
        "            np.concatenate([attr1Lengths, attr2Lengths]), np.concatenate([attribute1, attribute2]))\n",
        "    for word in topWordsT1[::-1]:\n",
        "        print(\"\\t\"+word + \", \" + trans.translate(word))\n",
        "\n",
        "    print()\n",
        "    print(\"Top 5 most similar attribute words to %s:\" % target2Name + \", \" + trans.translate(target2Name))\n",
        "\n",
        "    topWordsT2 = rankAttributes(target2Vecs, target2Lengths, np.concatenate([attr1Data, attr2Data]),\n",
        "            np.concatenate([attr1Lengths, attr2Lengths]), np.concatenate([attribute1, attribute2]))\n",
        "    for word in topWordsT2[::-1]:\n",
        "        print(\"\\t\"+word + \", \" + trans.translate(word))\n",
        "\n",
        "    print()\n",
        "    #calculate similarities between target 1 and both attributes\n",
        "    targ1attr1Sims = [getAverageSimilarity( target1Vecs[i], target1Lengths[i], attr1Data, attr1Lengths)\n",
        "        for i in range( target1Lengths.shape[0])]\n",
        "    targ1attr2Sims = [getAverageSimilarity( target1Vecs[i], target1Lengths[i], attr2Data, attr2Lengths)\n",
        "        for i in range( target1Lengths.shape[0])]\n",
        "    targ1SimDiff = np.subtract(targ1attr1Sims, targ1attr2Sims)\n",
        "\n",
        "    #calculate similarities between target 2 and both attributes\n",
        "    targ2attr1Sims = [getAverageSimilarity( target2Vecs[i], target2Lengths[i], attr1Data, attr1Lengths)\n",
        "        for i in range( target2Lengths.shape[0])]\n",
        "    targ2attr2Sims = [getAverageSimilarity( target2Vecs[i], target2Lengths[i], attr2Data, attr2Lengths)\n",
        "        for i in range( target2Lengths.shape[0])]\n",
        "    targ2SimDiff = np.subtract(targ2attr1Sims, targ2attr2Sims)\n",
        "\n",
        "    #effect size is avg difference in similarities divided by standard dev\n",
        "    d = (np.average(targ1SimDiff) - np.average(targ2SimDiff))/np.std(np.concatenate((targ1SimDiff,targ2SimDiff)))\n",
        "\n",
        "\n",
        "    print()\n",
        "    print(\"Calculating effect size.  The score is between +2.0 and -2.0.  \")\n",
        "    print(\"Positive scores indicate that %s is more associated with %s than %s.\" % (target1Name, attribute1Name, target2Name))\n",
        "    print(\"Or, equivalently, %s is more associated with %s than %s.\" % (target2Name, attribute2Name, target1Name))\n",
        "    print(\"Negative scores have the opposite relationship.\")\n",
        "    print(\"Scores close to 0 indicate little to no effect.\")\n",
        "    print()\n",
        "    print(\"Effect size: %.2f\" % d)\n",
        "\n",
        "    print()\n",
        "    print(\"Plotting similarity scores...\")\n",
        "\n",
        "    fig = plt.figure(figsize=(16,8))\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    ax1.set_title(\"Similarities Scores for Target/Attribute Pairs\")\n",
        "    ax2.set_title(\"Difference Scores For Each Target\")\n",
        "\n",
        "    # Box plot of pairwise similarity scores\n",
        "    df = pd.DataFrame()\n",
        "    df[\"Similarity\"] = np.concatenate([targ1attr1Sims,targ1attr2Sims,targ2attr1Sims,targ2attr2Sims])\n",
        "    df[\"Pairs\"] = [target1Name+\"-\"+attribute1Name]*len(targ1attr1Sims)+[target1Name+\"-\"+attribute2Name]*len(targ1attr2Sims)+[target2Name+\"-\"+attribute1Name]*len(targ2attr1Sims) \\\n",
        "      +[target2Name+\"-\"+attribute2Name]*len(targ2attr2Sims)\n",
        "    df[\"Target\"] = [target1Name]*len(targ1attr1Sims+targ1attr2Sims)+[target2Name]*len(targ2attr1Sims+targ2attr2Sims)\n",
        "    df[\"Attribute\"] = [attribute1Name]*len(targ1attr1Sims)+[attribute2Name]*len(targ1attr2Sims)+[attribute1Name]*len(targ2attr1Sims) +[attribute2Name]*len(targ2attr2Sims)\n",
        "    sns.boxplot(x=\"Target\", y=\"Similarity\", hue=\"Attribute\",data=df, ax=ax1)\n",
        "    #Box plot of target bias in similarities\n",
        "    df = pd.DataFrame()\n",
        "    df[\"Difference\"] = np.concatenate([targ1SimDiff, targ2SimDiff])\n",
        "    df[\"Target\"] = [target1Name]*len(targ1SimDiff) + [target2Name]*len(targ2SimDiff)\n",
        "    ax = sns.boxplot(x=\"Target\", y=\"Difference\", data=df, ax=ax2)\n",
        "\n",
        "\n",
        "    ticks = ax1.get_yticks()\n",
        "    mx = max(abs(ticks[0]),ticks[-1])\n",
        "    mx = int(mx*10+.99)/10.0\n",
        "    ax1.yaxis.set_ticks(np.arange(-mx,mx+.1,.1))\n",
        "    ticks = ax2.get_yticks()\n",
        "    mx = max(abs(ticks[0]),ticks[-1])\n",
        "    mx = int(mx*10+.99)/10.0\n",
        "    ax2.yaxis.set_ticks(np.arange(-mx,mx+.1,.1))\n",
        "\n",
        "    fig.subplots_adjust(wspace=0.5)\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    labels = [item.get_text() for item in ax1.get_yticklabels()]\n",
        "    labels[0] = \"(less similar) \" + labels[0]\n",
        "    labels[-1] = \"(more similar) \" + labels[-1]\n",
        "    ax1.set_yticklabels(labels)\n",
        "    labels = [item.get_text() for item in ax2.get_yticklabels()]\n",
        "    labels[0] = \"(%s) \" % attribute2Name + labels[0]\n",
        "    labels[len(labels)//2] = \"(neutral) 0.0\"\n",
        "    labels[-1] = \"(%s) \" % attribute1Name + labels[-1]\n",
        "    ax2.set_yticklabels(labels)\n",
        "\n",
        "    fig.savefig(test_name+'.png')\n",
        "\n",
        "    sys.stdout = orig_stdout\n",
        "    f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MpM3hoEEnHHn",
        "outputId": "6edabb72-c7d5-46fd-8bc5-487af95f073d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"gpt_ft\", 'zip', \"gpt2-ft/\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Fine-tune a non-English GPT-2 Model with Huggingface",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0113545a40914d959af6e1df749165f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c9a242a5bc041a8a8859f7edb5bbb63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b7d6a402dc4eeb8f1ba4a572929c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79314b959d4842da8841b69d411669cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ad9bd00ee22492fac41027cc5bea9c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "927e13f1feec4dd886ea47445e30eaa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79314b959d4842da8841b69d411669cd",
            "placeholder": "​",
            "style": "IPY_MODEL_0113545a40914d959af6e1df749165f0",
            "value": " 675M/675M [00:21&lt;00:00, 31.0MB/s]"
          }
        },
        "962f31f7eeb34fbe8fb49b3e411a0892": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b27e43aaebd243a286b0f47ed9fe3515": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_962f31f7eeb34fbe8fb49b3e411a0892",
            "placeholder": "​",
            "style": "IPY_MODEL_ba63820eff8c4a5e97e473dcaff87ea9",
            "value": "Downloading: 100%"
          }
        },
        "b4052e8352b84bb095b0ddd274ea0caa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b27e43aaebd243a286b0f47ed9fe3515",
              "IPY_MODEL_c0425becc3a14fc9a9ed1e0b5eafdc9f",
              "IPY_MODEL_927e13f1feec4dd886ea47445e30eaa7"
            ],
            "layout": "IPY_MODEL_2c9a242a5bc041a8a8859f7edb5bbb63"
          }
        },
        "ba63820eff8c4a5e97e473dcaff87ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0425becc3a14fc9a9ed1e0b5eafdc9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ad9bd00ee22492fac41027cc5bea9c8",
            "max": 675497284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74b7d6a402dc4eeb8f1ba4a572929c4e",
            "value": 675497284
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
